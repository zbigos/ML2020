{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JGmPStV4yiw"
   },
   "source": [
    "# Lab Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUenJ9L141My"
   },
   "source": [
    "**Submission deadline:**\n",
    "* **Regular problems: last lab session before Friday, 6.11.20**\n",
    "* **Bonus problems: deadline for Lab Assignment 3**\n",
    "\n",
    "**Points: 12 + 3-6 bonus points**\n",
    "\n",
    "Please note: some of the assignments are tedious or boring if you are already a NumPy ninja. The bonus problems were designed to give you a more satisfying alternative.\n",
    "\n",
    "The assignment is in the form of a Jupyter notebook. We will be using [Google Colab](https://colab.research.google.com) to solve it. Below you will find a \"Setup\" section. Follow instructions from this paragraph to download the notebook and open it using [Google Colab](https://colab.research.google.com). \n",
    "\n",
    "Your goal is to solve problems posted below. Whenever possible, add your solutions to the notebook.\n",
    "\n",
    "Please email us about any problems with it - we will try to correct them quickly. Also, please do not hesitate to use GitHub’s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJXAnzVN2fgh"
   },
   "source": [
    "## Heads Up!\n",
    "\n",
    "This assignment comes with starter code, but you are not forced to use it, as long as you execute all analysis demanded in the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NsnbuW1uzVcC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/zbigniew/Devel/UWR/ML2020/MLvirtualenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Please note that this code needs only to be run in a fresh runtime.\n",
    "# However, it can be rerun afterwards too.\n",
    "!pip install -q gdown httpimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a4TIgG0bwlpS"
   },
   "outputs": [],
   "source": [
    "# Standard IPython notebook imports\n",
    "import itertools\n",
    "import io\n",
    "import os\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as sopt\n",
    "import scipy.stats as sstats\n",
    "import seaborn as sns\n",
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "from sklearn import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#import graphviz\n",
    "import httpimport\n",
    "\n",
    "# In this way we can import functions straight from github\n",
    "with httpimport.github_repo(\n",
    "    \"janchorowski\", \"nn_assignments\", module=\"common\", branch=\"nn18\"\n",
    "):\n",
    "    from common.gradients import check_gradient\n",
    "    from common.plotting import plot_mat\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7qCaa3LRuzJ"
   },
   "source": [
    "# Problem 1 [2p] Naive Bayes Classifier\n",
    "\n",
    "The Bayes' theorem allows us to construct a classifier in which we\n",
    "model how the data is generated. Here we will describe a\n",
    "simple and popular example of such a classifier called the naive\n",
    "Bayes classifier.  Despite its simplicity It is quite effective for\n",
    "classification of text documents (e.g. as spam and non-spam).\n",
    "\n",
    "Let a document be a sequence of tokens, which usually are words, but can also be characters: $D=W_1,W_2,\\ldots,W_n$ \n",
    "We will model generation of text documents as a two-stage process.\n",
    "First, document category $C_j$ is drawn at random with probability\n",
    "$p(C_j)$, also called the *a priori* probability.\n",
    "To define the class-conditional probability\n",
    "$p(D|C_j)$, we will make a simplifying (naive)\n",
    "assumption, that every token in the document is drawn independently at\n",
    "random with probability $p(W_i|C)$:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(D|C_j) = p(W_1,W_2,\\ldots,W_n | C_j) \\approx p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j).\n",
    "\\end{equation*}\n",
    "\n",
    "To infer the class of a document we apply the Bayes theorem:\n",
    "\\begin{equation*}\n",
    "p(C_j|D) = \\frac{p(D|C_j)p(C_j)}{p(D)} = \\frac{p(C_j)p(W_1|C_j)p(W_2|C_j)\\ldots p(W_n|C_j)}{p(D)}.\n",
    "\\end{equation*}\n",
    "Please note that since we assumed only a finite number of classes,\n",
    "we can compute the term $p(D)$ by making sure that the *a\n",
    "posteriori probabilities* $p(C_j|D)$ sum to $1$ over all classes.\n",
    "\n",
    "In this exercise we will try to mimic the language-guessing feature\n",
    "of [Google Translate](https://translate.google.com/), although\n",
    "on a much smaller scale.  We are given an input which is a\n",
    "lower-case sequence of characters (such as *\"some people like\n",
    "pineapple on their pizza\"*), and we determine whether the\n",
    "sequence's language is English, Polish or Spanish.\n",
    "We will treat each character as a separate observation.\n",
    "The numbers are taken from [Wikipedia article on letter frequency](https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_other_languages). We display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 6550,
     "status": "ok",
     "timestamp": 1603379296947,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "N-8g1QuNbIfs",
    "outputId": "5b290e79-b838-482e-c374-d14d09ce78db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>German</th>\n",
       "      <th>Spanish</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>Esperanto</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Turkish</th>\n",
       "      <th>Swedish</th>\n",
       "      <th>Polish</th>\n",
       "      <th>Dutch</th>\n",
       "      <th>Danish</th>\n",
       "      <th>Icelandic</th>\n",
       "      <th>Finnish</th>\n",
       "      <th>Czech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>8.167</td>\n",
       "      <td>7.636</td>\n",
       "      <td>6.516</td>\n",
       "      <td>11.525</td>\n",
       "      <td>14.634</td>\n",
       "      <td>12.117</td>\n",
       "      <td>11.745</td>\n",
       "      <td>12.920</td>\n",
       "      <td>9.383</td>\n",
       "      <td>10.503</td>\n",
       "      <td>7.486</td>\n",
       "      <td>6.025</td>\n",
       "      <td>10.110</td>\n",
       "      <td>12.217</td>\n",
       "      <td>8.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1.492</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.886</td>\n",
       "      <td>2.215</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.927</td>\n",
       "      <td>2.844</td>\n",
       "      <td>1.535</td>\n",
       "      <td>1.740</td>\n",
       "      <td>1.584</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>2.782</td>\n",
       "      <td>3.260</td>\n",
       "      <td>2.732</td>\n",
       "      <td>4.019</td>\n",
       "      <td>3.882</td>\n",
       "      <td>0.776</td>\n",
       "      <td>4.501</td>\n",
       "      <td>1.463</td>\n",
       "      <td>1.486</td>\n",
       "      <td>3.895</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>4.253</td>\n",
       "      <td>3.669</td>\n",
       "      <td>5.076</td>\n",
       "      <td>5.010</td>\n",
       "      <td>4.992</td>\n",
       "      <td>3.044</td>\n",
       "      <td>3.736</td>\n",
       "      <td>5.206</td>\n",
       "      <td>4.702</td>\n",
       "      <td>3.725</td>\n",
       "      <td>5.933</td>\n",
       "      <td>5.858</td>\n",
       "      <td>1.575</td>\n",
       "      <td>1.043</td>\n",
       "      <td>3.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>12.702</td>\n",
       "      <td>14.715</td>\n",
       "      <td>16.396</td>\n",
       "      <td>12.181</td>\n",
       "      <td>12.570</td>\n",
       "      <td>8.995</td>\n",
       "      <td>11.792</td>\n",
       "      <td>9.912</td>\n",
       "      <td>10.149</td>\n",
       "      <td>7.352</td>\n",
       "      <td>18.910</td>\n",
       "      <td>15.453</td>\n",
       "      <td>6.418</td>\n",
       "      <td>7.968</td>\n",
       "      <td>7.562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English  French  German  Spanish  Portuguese  Esperanto  Italian  Turkish  \\\n",
       "a    8.167   7.636   6.516   11.525      14.634     12.117   11.745   12.920   \n",
       "b    1.492   0.901   1.886    2.215       1.043      0.980    0.927    2.844   \n",
       "c    2.782   3.260   2.732    4.019       3.882      0.776    4.501    1.463   \n",
       "d    4.253   3.669   5.076    5.010       4.992      3.044    3.736    5.206   \n",
       "e   12.702  14.715  16.396   12.181      12.570      8.995   11.792    9.912   \n",
       "\n",
       "   Swedish  Polish   Dutch  Danish  Icelandic  Finnish  Czech  \n",
       "a    9.383  10.503   7.486   6.025     10.110   12.217  8.421  \n",
       "b    1.535   1.740   1.584   2.000      1.043    0.281  0.822  \n",
       "c    1.486   3.895   1.242   0.565      0.000    0.281  0.740  \n",
       "d    4.702   3.725   5.933   5.858      1.575    1.043  3.475  \n",
       "e   10.149   7.352  18.910  15.453      6.418    7.968  7.562  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_table = u\"\"\"English|French|German|Spanish|Portuguese|Esperanto|Italian|Turkish|Swedish|Polish|Dutch|Danish|Icelandic|Finnish|Czech\n",
    "a|8.167|7.636|6.516|11.525|14.634|12.117|11.745|12.920|9.383|10.503|7.486|6.025|10.110|12.217|8.421\n",
    "b|1.492|0.901|1.886|2.215|1.043|0.980|0.927|2.844|1.535|1.740|1.584|2.000|1.043|0.281|0.822\n",
    "c|2.782|3.260|2.732|4.019|3.882|0.776|4.501|1.463|1.486|3.895|1.242|0.565|0|0.281|0.740\n",
    "d|4.253|3.669|5.076|5.010|4.992|3.044|3.736|5.206|4.702|3.725|5.933|5.858|1.575|1.043|3.475\n",
    "e|12.702|14.715|16.396|12.181|12.570|8.995|11.792|9.912|10.149|7.352|18.91|15.453|6.418|7.968|7.562\n",
    "f|2.228|1.066|1.656|0.692|1.023|1.037|1.153|0.461|2.027|0.143|0.805|2.406|3.013|0.194|0.084\n",
    "g|2.015|0.866|3.009|1.768|1.303|1.171|1.644|1.253|2.862|1.731|3.403|4.077|4.241|0.392|0.092\n",
    "h|6.094|0.737|4.577|0.703|0.781|0.384|0.636|1.212|2.090|1.015|2.380|1.621|1.871|1.851|1.356\n",
    "i|6.966|7.529|6.550|6.247|6.186|10.012|10.143|9.600|5.817|8.328|6.499|6.000|7.578|10.817|6.073\n",
    "j|0.153|0.613|0.268|0.493|0.397|3.501|0.011|0.034|0.614|1.836|1.46|0.730|1.144|2.042|1.433\n",
    "k|0.772|0.049|1.417|0.011|0.015|4.163|0.009|5.683|3.140|2.753|2.248|3.395|3.314|4.973|2.894\n",
    "l|4.025|5.456|3.437|4.967|2.779|6.104|6.510|5.922|5.275|2.564|3.568|5.229|4.532|5.761|3.802\n",
    "m|2.406|2.968|2.534|3.157|4.738|2.994|2.512|3.752|3.471|2.515|2.213|3.237|4.041|3.202|2.446\n",
    "n|6.749|7.095|9.776|6.712|4.446|7.955|6.883|7.987|8.542|6.237|10.032|7.240|7.711|8.826|6.468\n",
    "o|7.507|5.796|2.594|8.683|9.735|8.779|9.832|2.976|4.482|6.667|6.063|4.636|2.166|5.614|6.695\n",
    "p|1.929|2.521|0.670|2.510|2.523|2.755|3.056|0.886|1.839|2.445|1.57|1.756|0.789|1.842|1.906\n",
    "q|0.095|1.362|0.018|0.877|1.204|0|0.505|0|0.020|0|0.009|0.007|0|0.013|0.001\n",
    "r|5.987|6.693|7.003|6.871|6.530|5.914|6.367|7.722|8.431|5.243|6.411|8.956|8.581|2.872|4.799\n",
    "s|6.327|7.948|7.270|7.977|6.805|6.092|4.981|3.014|6.590|5.224|3.73|5.805|5.630|7.862|5.212\n",
    "t|9.056|7.244|6.154|4.632|4.336|5.276|5.623|3.314|7.691|2.475|6.79|6.862|4.953|8.750|5.727\n",
    "u|2.758|6.311|4.166|2.927|3.639|3.183|3.011|3.235|1.919|2.062|1.99|1.979|4.562|5.008|2.160\n",
    "v|0.978|1.838|0.846|1.138|1.575|1.904|2.097|0.959|2.415|0.012|2.85|2.332|2.437|2.250|5.344\n",
    "w|2.360|0.074|1.921|0.017|0.037|0|0.033|0|0.142|5.813|1.52|0.069|0|0.094|0.016\n",
    "x|0.150|0.427|0.034|0.215|0.253|0|0.003|0|0.159|0.004|0.036|0.028|0.046|0.031|0.027\n",
    "y|1.974|0.128|0.039|1.008|0.006|0|0.020|3.336|0.708|3.206|0.035|0.698|0.900|1.745|1.043\n",
    "z|0.074|0.326|1.134|0.467|0.470|0.494|1.181|1.500|0.070|4.852|1.39|0.034|0|0.051|1.503\n",
    "à|0|0.486|0|0|0.072|0|0.635|0|0|0|0|0|0|0|0\n",
    "â|0|0.051|0|0|0.562|0|0|0|0|0|0|0|0|0|0\n",
    "á|0|0|0|0.502|0.118|0|0|0|0|0|0|0|1.799|0|0.867\n",
    "å|0|0|0|0|0|0|0|0|1.338|0|0|1.190|0|0.003|0\n",
    "ä|0|0|0.578|0|0|0|0|0|1.797|0|0|0|0|3.577|0\n",
    "ã|0|0|0|0|0.733|0|0|0|0|0|0|0|0|0|0\n",
    "ą|0|0|0|0|0|0|0|0|0|0.699|0|0|0|0|0\n",
    "æ|0|0|0|0|0|0|0|0|0|0|0|0.872|0.867|0|0\n",
    "œ|0|0.018|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ç|0|0.085|0|0|0.530|0|0|1.156|0|0|0|0|0|0|0\n",
    "ĉ|0|0|0|0|0|0.657|0|0|0|0|0|0|0|0|0\n",
    "ć|0|0|0|0|0|0|0|0|0|0.743|0|0|0|0|0\n",
    "č|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.462\n",
    "ď|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.015\n",
    "ð|0|0|0|0|0|0|0|0|0|0|0|0|4.393|0|0\n",
    "è|0|0.271|0|0|0|0|0.263|0|0|0|0|0|0|0|0\n",
    "é|0|1.504|0|0.433|0.337|0|0|0|0|0|0|0|0.647|0|0.633\n",
    "ê|0|0.218|0|0|0.450|0|0|0|0|0|0|0|0|0|0\n",
    "ë|0|0.008|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ę|0|0|0|0|0|0|0|0|0|1.035|0|0|0|0|0\n",
    "ě|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1.222\n",
    "ĝ|0|0|0|0|0|0.691|0|0|0|0|0|0|0|0|0\n",
    "ğ|0|0|0|0|0|0|0|1.125|0|0|0|0|0|0|0\n",
    "ĥ|0|0|0|0|0|0.022|0|0|0|0|0|0|0|0|0\n",
    "î|0|0.045|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ì|0|0|0|0|0|0|0.030|0|0|0|0|0|0|0|0\n",
    "í|0|0|0|0.725|0.132|0|0|0|0|0|0|0|1.570|0|1.643\n",
    "ï|0|0.005|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ı|0|0|0|0|0|0|0|5.114|0|0|0|0|0|0|0\n",
    "ĵ|0|0|0|0|0|0.055|0|0|0|0|0|0|0|0|0\n",
    "ł|0|0|0|0|0|0|0|0|0|2.109|0|0|0|0|0\n",
    "ñ|0|0|0|0.311|0|0|0|0|0|0|0|0|0|0|0\n",
    "ń|0|0|0|0|0|0|0|0|0|0.362|0|0|0|0|0\n",
    "ň|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.007\n",
    "ò|0|0|0|0|0|0|0.002|0|0|0|0|0|0|0|0\n",
    "ö|0|0|0.443|0|0|0|0|0.777|1.305|0|0|0|0.777|0.444|0\n",
    "ô|0|0.023|0|0|0.635|0|0|0|0|0|0|0|0|0|0\n",
    "ó|0|0|0|0.827|0.296|0|0|0|0|1.141|0|0|0.994|0|0.024\n",
    "õ|0|0|0|0|0.040|0|0|0|0|0|0|0|0|0|0\n",
    "ø|0|0|0|0|0|0|0|0|0|0|0|0.939|0|0|0\n",
    "ř|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.380\n",
    "ŝ|0|0|0|0|0|0.385|0|0|0|0|0|0|0|0|0\n",
    "ş|0|0|0|0|0|0|0|1.780|0|0|0|0|0|0|0\n",
    "ś|0|0|0|0|0|0|0|0|0|0.814|0|0|0|0|0\n",
    "š|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.688\n",
    "ß|0|0|0.307|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ť|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.006\n",
    "þ|0|0|0|0|0|0|0|0|0|0|0|0|1.455|0|0\n",
    "ù|0|0.058|0|0|0|0|0.166|0|0|0|0|0|0|0|0\n",
    "ú|0|0|0|0.168|0.207|0|0|0|0|0|0|0|0.613|0|0.045\n",
    "û|0|0.060|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "ŭ|0|0|0|0|0|0.520|0|0|0|0|0|0|0|0|0\n",
    "ü|0|0|0.995|0.012|0.026|0|0|1.854|0|0|0|0|0|0|0\n",
    "ů|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.204\n",
    "ý|0|0|0|0|0|0|0|0|0|0|0|0|0.228|0|0.995\n",
    "ź|0|0|0|0|0|0|0|0|0|0.078|0|0|0|0|0\n",
    "ż|0|0|0|0|0|0|0|0|0|0.706|0|0|0|0|0\n",
    "ž|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0.721\"\"\"\n",
    "df = pd.read_table(io.StringIO(wiki_table), sep=\"|\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Av5tmHDbKOn"
   },
   "source": [
    "Implement the language classifier and answer the following:\n",
    "\n",
    "1. **[0.5p]** Naive Bayes can be implemented\n",
    "    either by multiplying probabilities or by adding\n",
    "    log-probabilities. Which one is better and why?\n",
    "    \n",
    "    Please type a short answer below.\n",
    "2. **[1.5p]** What is the language of the following phrases, according to the Naive Bayes classifier (whihc you have to implement in a code cell below)? Assume equal prior language probabilities $P(C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qpm3aaICM-7"
   },
   "source": [
    "Using log-probabilities is **debatable** because **adding log-probabilities has an advantage of not handling really small values. However, multiplying probs has a given advantage, where we handle probabilities of exactly 0.0 (that throws math error in log based calculations)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 6540,
     "status": "ok",
     "timestamp": 1603379296948,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "6qqxHPSF6pQ0",
    "outputId": "b2ebd7c8-6809-4943-e664-663d31fe3909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: English,French,German,Spanish,Portuguese,Esperanto,Italian,Turkish,Swedish,Polish,Dutch,Danish,Icelandic,Finnish,Czech\n",
      "Letters: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, à, â, á, å, ä, ã, ą, æ, œ, ç, ĉ, ć, č, ď, ð, è, é, ê, ë, ę, ě, ĝ, ğ, ĥ, î, ì, í, ï, ı, ĵ, ł, ñ, ń, ň, ò, ö, ô, ó, õ, ø, ř, ŝ, ş, ś, š, ß, ť, þ, ù, ú, û, ŭ, ü, ů, ý, ź, ż, ž\n",
      "P(ę|Polish) = 1.035\n"
     ]
    }
   ],
   "source": [
    "# We can easily manipulate the letter frequency table using Pandas\n",
    "langs = list(df)\n",
    "letters = list(df.index)\n",
    "print(\"Languages:\", \",\".join(langs))\n",
    "print(\"Letters:\", \", \".join(letters))\n",
    "print(\"P(ę|Polish) =\", df.loc[\"ę\", \"Polish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "executionInfo": {
     "elapsed": 6539,
     "status": "ok",
     "timestamp": 1603379296950,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "kW6DtwD48nzD",
    "outputId": "1e0b0664-667b-47f1-ab0a-ee4fe20a7d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total letter count by language:\n",
      "English        99.999\n",
      "French        100.060\n",
      "German        100.002\n",
      "Spanish       100.000\n",
      "Portuguese    100.040\n",
      "Esperanto      99.960\n",
      "Italian       100.007\n",
      "Turkish       106.997\n",
      "Swedish        99.999\n",
      "Polish        100.027\n",
      "Dutch         100.157\n",
      "Danish         99.999\n",
      "Icelandic      99.998\n",
      "Finnish       100.004\n",
      "Czech          88.013\n",
      "dtype: float64\n",
      "\n",
      "After normalization:\n",
      "English       1.0\n",
      "French        1.0\n",
      "German        1.0\n",
      "Spanish       1.0\n",
      "Portuguese    1.0\n",
      "Esperanto     1.0\n",
      "Italian       1.0\n",
      "Turkish       1.0\n",
      "Swedish       1.0\n",
      "Polish        1.0\n",
      "Dutch         1.0\n",
      "Danish        1.0\n",
      "Icelandic     1.0\n",
      "Finnish       1.0\n",
      "Czech         1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The values are percentages of letter appearance, but curiously enough they don't\n",
    "# sum to 100%.\n",
    "print(f\"\\nTotal letter count by language:\\n{df.sum(0)}\")\n",
    "\n",
    "# Thus we normalize the data such that the letter frequencies add up to 1 for each language\n",
    "df_norm = df.div(df.sum(axis=0), axis=1)\n",
    "\n",
    "print(f\"\\nAfter normalization:\\n{df_norm.sum(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SZI1U6VgcfxL"
   },
   "outputs": [],
   "source": [
    "def naive_bayes(sent, langs, df):\n",
    "    \"\"\"Returns the most probable language of a sentence\"\"\"\n",
    "\n",
    "    # Try working with log-probabilities.\n",
    "    # to prevent taking log(0) you can e.g. add a very small amount (1e-100)\n",
    "    # to each tabulated frequency.\n",
    "    def softlog(x):\n",
    "        if x == 0.0:\n",
    "            x = 1e-100\n",
    "        return math.log(x)\n",
    "    \n",
    "    df_log = df.applymap(softlog)\n",
    "    # normalize the sentence: remove spaces and punctuations, take lower case\n",
    "    for eject in [\".\", \" \", \",\", \"'\"]:\n",
    "        sent = sent.replace(eject, \"\")\n",
    "    sent = sent.lower()\n",
    "\n",
    "    log_probs = {}\n",
    "    for lang in langs:\n",
    "        prob = 0.0\n",
    "        for letter in sent:\n",
    "            prob += df_log.loc[letter, lang]\n",
    "        log_probs[lang] = prob\n",
    "\n",
    "    probs = []\n",
    "    for l, p in log_probs.items():\n",
    "        probs.append((p, l))\n",
    "    \n",
    "    reee = {}\n",
    "    for p, l in sorted(probs, reverse = True):\n",
    "        reee[l] = p\n",
    "    \n",
    "    return reee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 7398,
     "status": "ok",
     "timestamp": 1603379297823,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "H2PK1SnFBZVc",
    "outputId": "38315e34-e90d-4c91-d900-fa9fb7e9b2dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dejes para mañana lo que puedas hacer hoy.:\n",
      "Spanish: -109.22389109425953\t\n",
      "English: -337.2990768199661\t\n",
      "Portuguese: -337.3786236106419\t\n",
      "French: -337.3941821201992\t\n",
      "Swedish: -341.704036001394\t\n",
      "Italian: -342.8689469564796\t\n",
      "Finnish: -343.44580960470415\t\n",
      "Dutch: -344.16539765024163\t\n",
      "Czech: -344.7096403586452\t\n",
      "Danish: -345.10098410102927\t\n",
      "German: -346.120321306024\t\n",
      "Polish: -563.8011698924389\t\n",
      "Turkish: -569.1950469885481\t\n",
      "Esperanto: -787.8944414548469\t\n",
      "Icelandic: -795.4361872020115\t\n",
      "\n",
      "\n",
      "Przed wyruszeniem w drogę należy zebrać drużynę.:\n",
      "Polish: -134.6169120461959\t\n",
      "English: -1273.34492114658\t\n",
      "Dutch: -1274.5249376000809\t\n",
      "German: -1274.779943697768\t\n",
      "Spanish: -1276.661350480558\t\n",
      "Czech: -1280.6576717849587\t\n",
      "Swedish: -1280.7459717660456\t\n",
      "French: -1282.049493348594\t\n",
      "Danish: -1282.2237174453614\t\n",
      "Italian: -1286.1344011369924\t\n",
      "Finnish: -1291.0389411447452\t\n",
      "Portuguese: -1292.1859636951629\t\n",
      "Turkish: -1716.8102013263986\t\n",
      "Esperanto: -2400.3731319533385\t\n",
      "Icelandic: -2401.780455788748\t\n",
      "\n",
      "\n",
      "Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.:\n",
      "Polish: -183.29071014561552\t\n",
      "Dutch: -1545.2119163419995\t\n",
      "Czech: -1548.4598751186513\t\n",
      "German: -1550.139504250054\t\n",
      "French: -1557.1314660778328\t\n",
      "Swedish: -1558.55442519859\t\n",
      "English: -1558.7490508390597\t\n",
      "Spanish: -1559.0460321092883\t\n",
      "Danish: -1560.4388863310373\t\n",
      "Finnish: -1561.9166297626964\t\n",
      "Portuguese: -1569.299238172609\t\n",
      "Italian: -1573.8296620188883\t\n",
      "Turkish: -1775.2626125494048\t\n",
      "Esperanto: -2219.088792796157\t\n",
      "Icelandic: -3126.414625060513\t\n",
      "\n",
      "\n",
      "Si vale la pena hacerlo vale la pena hacerlo bien.:\n",
      "Italian: -109.94738336509529\t\n",
      "Spanish: -112.93317944920054\t\n",
      "English: -114.8616049753579\t\n",
      "French: -114.9448286849575\t\n",
      "Dutch: -115.03353957980015\t\n",
      "Swedish: -115.31275818631562\t\n",
      "Portuguese: -115.54191710332415\t\n",
      "Esperanto: -116.59514889843075\t\n",
      "Czech: -117.62681634780007\t\n",
      "Danish: -118.67876763748819\t\n",
      "German: -119.29392245361656\t\n",
      "Turkish: -119.71907603928386\t\n",
      "Finnish: -119.96633544024574\t\n",
      "Polish: -131.0525223009279\t\n",
      "Icelandic: -574.5697777123687\t\n",
      "\n",
      "\n",
      "Experience is what you get when you didn't get what you wanted.:\n",
      "English: -147.79301048606928\t\n",
      "German: -163.78724858237274\t\n",
      "Dutch: -164.45071654843863\t\n",
      "Polish: -164.61123754419887\t\n",
      "Swedish: -169.14527037734763\t\n",
      "Danish: -173.3110687211666\t\n",
      "French: -174.76169100232715\t\n",
      "Finnish: -175.9735193686833\t\n",
      "Spanish: -177.48945703064615\t\n",
      "Czech: -186.51691119509542\t\n",
      "Italian: -189.4622462754832\t\n",
      "Portuguese: -190.2615806611365\t\n",
      "Turkish: -1293.4645286945574\t\n",
      "Icelandic: -1299.6153887656687\t\n",
      "Esperanto: -1970.1071056408261\t\n",
      "\n",
      "\n",
      "Należy prowokować intelekt, nie intelektualistów.:\n",
      "Polish: -135.32166007746721\t\n",
      "Czech: -599.3785494661803\t\n",
      "Spanish: -611.4289293233775\t\n",
      "Portuguese: -617.411786044161\t\n",
      "English: -807.6089277902902\t\n",
      "Dutch: -809.9387536676687\t\n",
      "Finnish: -810.1476810115971\t\n",
      "German: -814.3404928026141\t\n",
      "Swedish: -814.8754219272304\t\n",
      "Danish: -816.7503082607795\t\n",
      "French: -827.0763247067426\t\n",
      "Italian: -835.4235791882464\t\n",
      "Icelandic: -1266.9166621077964\t\n",
      "Turkish: -1488.6528064054214\t\n",
      "Esperanto: -1707.3041530689923\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"No dejes para mañana lo que puedas hacer hoy.\",\n",
    "    \"Przed wyruszeniem w drogę należy zebrać drużynę.\",\n",
    "    \"Żeby zrozumieć rekurencję, należy najpierw zrozumieć rekurencję.\",\n",
    "    \"Si vale la pena hacerlo vale la pena hacerlo bien.\",\n",
    "    \"Experience is what you get when you didn't get what you wanted.\",\n",
    "    \"Należy prowokować intelekt, nie intelektualistów.\",\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    print(f\"{sent}:\")\n",
    "    for k, v in naive_bayes(sent, langs, df_norm).items():\n",
    "        #if v < 1e-3:\n",
    "        #    break\n",
    "        print(f\"{k}: {v}\\t\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWzCDfvskvQQ"
   },
   "source": [
    "# Problem 2 [3p] Ridge Regression\n",
    "\n",
    "In this problem you will implement ridge regression **without using `sklearn`**!\n",
    "\n",
    "When the data set is small and highly dimensional (or when high degree polynomials are used) the linear regression solution may fit the noise in the data instead of capturing the general rule. We call this phenomenon overfitting and will discuss it in detail in a few lectures.\n",
    "\n",
    "One way of preventing overfitting is to force the model's parameters to be small. We call this *regularization*. Consider the following cost function:\n",
    "\n",
    "$$ J(\\Theta) = \\frac{1}{N}\\sum_{i=1}^N (y^{(i)} - x^{(i)}\\Theta)^2 + \\frac{\\alpha}{N} \\Theta^T \\Theta $$\n",
    "\n",
    "Analyze datasets sampled using the following procedure:\n",
    "\n",
    "1. $x \\propto U(0;1)$: first $x$ is sampled uniformly from the  $0-1$ range.\n",
    "2. $y \\propto \\mathcal{N}(\\mu=1+2x-5x^2 + 4x^3, \\sigma=0.1)$: then \n",
    "    $y$ is sampled from the Normal distribution with mean \n",
    "    $\\mu=1+2x-5x^2+4x^3$ and standard deviation $0.1$\n",
    "\n",
    "\n",
    "## Task 2.1 [1.5p]\n",
    "Repeat 30 times an experiment in which you sample a new training\n",
    "dataset, then fit polynomials of degree 0 to 14 and use $\\alpha$\n",
    "value from the set $\\{0, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$.\n",
    "\n",
    "Plot the mean training and testing errors. What is the effect of increasing $\\alpha$?\n",
    "\n",
    "Plot how the error rates depend on the the polynomial degree and regularization constant. Try to find the best value for alpha on the test set, **explain** the model behavoir for small alphas and large alphas.\n",
    "\n",
    "## Task 2.2 [0.5p]\n",
    "Use a small alpha for numerical stability (1e-6) and train the model on increasingly large training sets. Plot the training curves (train and test error rates versus amount of training data). What can you notice?\n",
    "\n",
    "## Task 2.3 [1p]\n",
    "\n",
    "Now let's change the data slightly:\n",
    "1. $x \\propto U(0;10)$\n",
    "2. $y \\propto \\mathcal{N}(\\mu=1+0.2x-0.05x^2 + 0.004x^3, \\sigma=0.1)$\n",
    "\n",
    "Try fitting the polynomial regression once again. Would it make sense to normalize the data after feature expansion?\n",
    "\n",
    "Which dataset is easier to fit? When is ridge regression most effective? \n",
    "\n",
    "Note: in real life, we may e.g. have a dataset with mixed units, like milimeters and kilometers. Data normalization is often an important preprocessing step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NgOPB_f312v"
   },
   "source": [
    "## Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 7389,
     "status": "ok",
     "timestamp": 1603379297825,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "7Xe-Ilp0kynI",
    "outputId": "3f89c901-7562-45d9-b55c-39639feaad83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe2e1532c10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# The true polynomial relation:\n",
    "# y(x) = 1 + 2x -5x^2 + 4x^3\n",
    "#\n",
    "# TODO: write down the proper coefficients\n",
    "#\n",
    "\n",
    "\n",
    "def powers_of_X(X, degree):\n",
    "    powers = np.arange(degree + 1).reshape(1, -1)\n",
    "    return X ** powers\n",
    "\n",
    "\n",
    "def compute_polynomial(X, Theta):\n",
    "    XP = powers_of_X(X, len(Theta) - 1)  # len(Theta) x N\n",
    "    Y = np.sum(Theta * XP, axis = 1)\n",
    "    return Y\n",
    "\n",
    "true_poly_theta = np.array([1.0, 2.0, -5.0, 4.0])\n",
    "\n",
    "def make_dataset(N, theta=true_poly_theta, sigma=0.1):\n",
    "    \"\"\" Sample a dataset \"\"\"\n",
    "    X = np.random.uniform(size=(N, 1))\n",
    "    Y_clean = compute_polynomial(X, theta)    \n",
    "    Y = Y_clean + np.random.randn(1, N) * sigma # tutaj zmiana N, 1 na 1, N\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "train_data = make_dataset(30)\n",
    "XX = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "YY = compute_polynomial(XX, true_poly_theta)\n",
    "plt.scatter(train_data[0], train_data[1], label=\"train data\", color=\"r\")\n",
    "plt.plot(XX, compute_polynomial(XX, true_poly_theta), label=\"ground truth\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6cESiv0k2Kr"
   },
   "outputs": [],
   "source": [
    "def poly_fit(data, degree, alpha):\n",
    "    \"Fit a polynomial of a given degree and weight decay parameter alpha\"\n",
    "    X = powers_of_X(data[0], degree)  # Matrix N x d\n",
    "    Y = data[1].reshape(-1, 1)  # Matrix N x 1\n",
    "    #\n",
    "    # TODO: implement the closed-form solution for Theta\n",
    "    #\n",
    "    # Please note that np.linalg.inv may be numerically unstable.\n",
    "    # It is better to use np.linalg.solve or even a QR decomposition.\n",
    "    #\n",
    "    Theta = TODO\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 8445,
     "status": "ok",
     "timestamp": 1603379298894,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "S6bBWSMHaq1c",
    "outputId": "0def79c7-16fb-43b8-f9f2-ac96b2080b35"
   },
   "outputs": [],
   "source": [
    "num_test_samples = 100\n",
    "num_train_samples = [30]\n",
    "alphas = [0.0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "degrees = range(15)\n",
    "num_repetitions = 30\n",
    "\n",
    "\n",
    "# sample a single test dataset for all experiments\n",
    "test_data = make_dataset(num_test_samples)\n",
    "results = []\n",
    "\n",
    "for (repetition, num_train, alpha, degree,) in itertools.product(\n",
    "    range(num_repetitions), num_train_samples, alphas, degrees\n",
    "):\n",
    "    train_data = make_dataset(num_train)\n",
    "    Theta = poly_fit(train_data, degree, alpha)\n",
    "    train_err = TODO\n",
    "    test_err = TODO\n",
    "    results.append(\n",
    "        {\n",
    "            \"repetition\": repetition,\n",
    "            \"num_train\": num_train,\n",
    "            \"alpha\": alpha,\n",
    "            \"degree\": degree,\n",
    "            \"dataset\": \"train\",\n",
    "            \"err_rate\": train_err,\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"repetition\": repetition,\n",
    "            \"num_train\": num_train,\n",
    "            \"alpha\": alpha,\n",
    "            \"degree\": degree,\n",
    "            \"dataset\": \"test\",\n",
    "            \"err_rate\": test_err,\n",
    "        }\n",
    "    )\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 25556,
     "status": "ok",
     "timestamp": 1603379316017,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "XIJkqMDSgD6v",
    "outputId": "5b90d870-9a10-4ab8-cbe6-8ced7edf2e74"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "#\n",
    "# Plot how the error rates depend on the the polynomial degree and regularization\n",
    "# constant.\n",
    "# Try to find the best value for lambda on the test set, explain the model\n",
    "# behavoir for small lambdas and large lambdas.\n",
    "#\n",
    "# Hint: the plots below all use sns.relplot!\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX6O_YgL9Kc-"
   },
   "source": [
    "**TODO**\n",
    "\n",
    "Explain below the model behavoir for small alphas and large alphas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "romsFu8A9Qr1"
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5euD33Q362V"
   },
   "source": [
    "## Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 86036,
     "status": "ok",
     "timestamp": 1603379376507,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "c6KxJOQjf3rZ",
    "outputId": "5b4e8d43-9f33-4ae1-96bb-336807c3123e"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Now set a small regularizatoin for numerical stability  (e.g. alpha=1e-6)\n",
    "# and present the relationship between\n",
    "# train and test error rates for varous degrees of the polynomial for\n",
    "# different sizes of the train set.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfLeVb0Q4CK8"
   },
   "source": [
    "**TODO** Describe in two sentences the plot above. What can you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URVW15wKzAqe"
   },
   "source": [
    "## Task 2.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lCjVWNcBn9H"
   },
   "outputs": [],
   "source": [
    "# TODO: solve Task 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 103806,
     "status": "ok",
     "timestamp": 1603379394294,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "7QH53--Y9auj",
    "outputId": "a08058be-7973-44d4-8d3b-624094ee290b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 163994,
     "status": "ok",
     "timestamp": 1603379454493,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "WWh2t5Jr-yZX",
    "outputId": "17a82275-063d-4646-e663-54a73e309ecf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UjN0BD39epa"
   },
   "source": [
    "**TODO** Answer below the questions:\n",
    "\n",
    "Would it make sense to normalize the data after feature expansion?\n",
    "\n",
    "Which dataset is easier to fit?\n",
    "\n",
    "When is ridge regression most effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhUGvZvlk6hU"
   },
   "source": [
    "# Problem 3 [1p + 2bp] Numerical optimization\n",
    "\n",
    "Implement the [Rosenborck function](https://en.wikipedia.org/wiki/Rosenbrock_function). Then find its optimum using the [`scipy.optimize.fmin_l_bfgs_b`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXNY375lmI1e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Implement the Rosenbrock function\n",
    "#\n",
    "\n",
    "\n",
    "def rosenbrock_v(x):\n",
    "    \"\"\"Returns the value of Rosenbrock's function at x\"\"\"\n",
    "    return TOOD\n",
    "\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"Returns the value of rosenbrock's function and its gradient at x\n",
    "    \"\"\"\n",
    "    val = TODO\n",
    "    # Gradient should be np.array\n",
    "    dVdX= TODO\n",
    "    return [val, dVdX]\n",
    "\n",
    "\n",
    "#\n",
    "# Feel free to add your own test points.\n",
    "#\n",
    "for test_point in [[0.0, 0.0], [1.0, 1.0], [0.5, 1.0], [1.0, 0.5]]:\n",
    "    assert check_gradient(rosenbrock, np.array(test_point), prec=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 163986,
     "status": "ok",
     "timestamp": 1603379454499,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "oe7hT5FgmfoZ",
    "outputId": "788c6342-9d77-4686-9c20-d2805062eb71"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Use scipy.optimize.fmin_l_bfgs_b\n",
    "# Make a contour plot of the Rosenbrock function, plot the optimization path\n",
    "# on the coutnour plot\n",
    "#\n",
    "# Hint: to save the points you can use the callback argument!\n",
    "#\n",
    "\n",
    "lbfsg_hist = []\n",
    "\n",
    "\n",
    "def save_hist(x):\n",
    "    lbfsg_hist.append(np.array(x))\n",
    "\n",
    "\n",
    "x_start = [0.0, 2.0]\n",
    "lbfsgb_ret = sopt.fmin_l_bfgs_b(\n",
    "    rosenbrock, x_start, callback=save_hist\n",
    "lbfsgb_ret = sopt.fmin_l_bfgs_b(TODO, TODO, callback=save_hist)\n",
    "\n",
    "# TODO: plot the countours of the function and overlay the optimization trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "executionInfo": {
     "elapsed": 164591,
     "status": "ok",
     "timestamp": 1603379455114,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "qWefLs0Hmzhc",
    "outputId": "e65b94b0-ec37-4f63-94ba-ef505883ddae"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Bonus problem\n",
    "#\n",
    "\n",
    "# Newton-Raphson Method\n",
    "\n",
    "\n",
    "def rosenbrock_hessian(x):\n",
    "    # TODO: compute the value, gradient and Hessian of Rosenbrock's function'\n",
    "        return [val, np.array((dvdx0, dvdx1)), H]\n",
    "\n",
    "\n",
    "def Newton(f, Theta0, alpha, stop_tolerance=1e-10, max_steps=1000000):\n",
    "\n",
    "    # TODO:\n",
    "    #  - implement the newton method and a simple line search\n",
    "    #  - make sure your function is resilient at critical points (such as seddle points)\n",
    "    #  - if the Newton direction is not minimizing the function, use the gradient for a few steps\n",
    "    #  - try to beat L-BFGS on the bmber of function evaluations needed!\n",
    "\n",
    "        return Theta, history, fun_evals\n",
    "\n",
    "\n",
    "Xopt, Xhist, fun_evals = Newton(\n",
    "    rosenbrock_hessian, x_start, alpha=1e-0, stop_tolerance=1e-10, max_steps=1e6\n",
    ")\n",
    "Xhist_ = np.array([[x[0][0], x[0][1]] for x in Xhist])\n",
    "\n",
    "print(\n",
    "    \"Found optimum at %s in %d steps (%d function evals)(true minimum is at [1,1])\"\n",
    "    % (Xopt, len(Xhist), fun_evals)\n",
    ")\n",
    "\n",
    "MX, MY = np.meshgrid(np.linspace(-1, 2, 100), np.linspace(-1, 2, 100))\n",
    "Z = np.array([MX, MY]).reshape(2, -1)\n",
    "VR = rosenbrock_v(Z)\n",
    "plt.contour(MX, MY, VR.reshape(MX.shape), 100)\n",
    "plt.plot(Xhist_[:, 0], Xhist_[:, 1], \"*-k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhEQD6LVrAbB"
   },
   "source": [
    "# Problem 4 (Logistic Regression) [2p]\n",
    "\n",
    "Linear regression is suitable for problems, where\n",
    "the forecasted values are real numbers. We use logistic regression,\n",
    "when we want to label the data with $0$ and $1$.\n",
    "\n",
    "Let $x\\in \\mathbb{R}^n$ be a row vector of $n$ real numbers, and\n",
    "$y\\in \\{0,1\\}$ the given class label. Similarly to what was shown\n",
    "during the lecture, we add an additional element $x_0=1$\n",
    "to vector $x$, to account for the bias term (and simplify the equations).\n",
    "\n",
    "Similarly to linear regression, vector\n",
    "$\\Theta\\in \\mathbb{R}^{n+1}$ parametrizes the model\n",
    "($n$ coefficients describes the data, the remaining one is the intercept).\n",
    "In logistic regression, we model conditional probability that\n",
    "sample $x$ belongs to class $1$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\text{class}=1|x, \\Theta)=h_\\Theta(x) = \\sigma\\left(\\sum_{j=0}^n  x_j\\Theta_j\\right) \n",
    "= \\sigma\\left(x\\Theta \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma(a) = \\frac{1}{1+\\exp(-a)}$ is being called the logistic sigmoid\n",
    "(a function, which plot is s-curved).\n",
    "\n",
    "An unknown sample $x$ is being labeled $1$ if\n",
    "$h_\\Theta(x)\\geq 0.5$, or equivalently, $x\\Theta \\geq 0$.\n",
    "\n",
    "Classification mismatch between the forecasted values and\n",
    "the data is being measured most of the time with cross-entropy:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\Theta) = - \\sum_{i=1}^m y^{(i)} \\log \\left(h_\\Theta (x^{(i)})\\right) + (1-y^{(i)}) \\log \\left(1-h_\\Theta (x^{(i)})\\right),\n",
    "\\end{equation}\n",
    "\n",
    "assuming $0\\log(0)=0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3adbOOZQwfRX"
   },
   "source": [
    "## Task 4.1 [1p]\n",
    "Use logistic regression to distinguish\n",
    "  _Versicolor_ and _Virginica_ irises. Use only the\n",
    "  `petal length` and `petal width` features. Use either\n",
    "  Gradient Descent, or L-BFGS to solve for the optimal $\\Theta$.\n",
    "  Prepare the scatterplot of the data and plot the class separation\n",
    "  boundary found by logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVX3t6huvLYN"
   },
   "source": [
    "## Task 4.2 [1p]\n",
    "\n",
    "In an analogy to Ridge Regression, implement a penalty on the weights of logistic regression. \n",
    "\n",
    "Then make a contour plot of the probability assigned by the model with no regularization $\\alpha=0$ and with regularization, e.g. $\\alpha=10^-2$ and $\\alpha=1$. Describe what has changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTgxDfWOxHTa"
   },
   "source": [
    "## Task 4.1 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1603388645508,
     "user": {
      "displayName": "Michał Stypułkowski",
      "photoUrl": "",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "hyCHThWMq8po",
    "outputId": "37abdeb5-f730-4412-b1c4-7055fcbae89d"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(\"Features: \", iris.feature_names)\n",
    "print(\"Targets: \", iris.target_names)\n",
    "petal_length = iris.data[:, iris.feature_names.index(\"petal length (cm)\")].reshape(-1, 1)\n",
    "petal_width = iris.data[:, iris.feature_names.index(\"petal width (cm)\")].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 2521,
     "status": "ok",
     "timestamp": 1603388648425,
     "user": {
      "displayName": "Michał Stypułkowski",
      "photoUrl": "",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "cJo8EKkxrDec",
    "outputId": "b85c9976-db78-4a3f-db47-a75acb51a434"
   },
   "outputs": [],
   "source": [
    "# Extract the petal_length and petal_width of versicolors and virginicas\n",
    "\n",
    "IrisX = np.hstack([np.ones_like(petal_length), petal_length, petal_width])\n",
    "IrisX = IrisX[iris.target != 0, :]\n",
    "\n",
    "# Set versicolor=0 and virginia=1\n",
    "IrisY = (iris.target[iris.target != 0] - 1).reshape(-1, 1).astype(np.float64)\n",
    "\n",
    "plt.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY, cmap=\"spring\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1603388651478,
     "user": {
      "displayName": "Michał Stypułkowski",
      "photoUrl": "",
      "userId": "08499681120788190249"
     },
     "user_tz": -120
    },
    "id": "lg6jTAsxrQrU",
    "outputId": "ecb22311-64da-4f3c-e012-a5b9d265538b"
   },
   "outputs": [],
   "source": [
    "def logreg_loss(Theta, X, Y):\n",
    "    #\n",
    "    # Write a logistic regression cost suitable for use with fmin_l_bfgs\n",
    "    #\n",
    "\n",
    "    # reshape Theta into a column vector - lBFGS gives us a flat array\n",
    "    ThetaR = Theta.reshape(X.shape[1], 1)\n",
    "\n",
    "    nll = TODO\n",
    "    grad = TODO\n",
    "\n",
    "    # reshape grad into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return nll, grad.reshape(Theta.shape)\n",
    "\n",
    "\n",
    "Theta0 = np.zeros((3, ))\n",
    "\n",
    "#\n",
    "# Call a solver\n",
    "#\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(\n",
    "    lambda Theta: logreg_loss(Theta, IrisX, IrisY), np.array(Theta0)\n",
    ")[0]\n",
    "\n",
    "\n",
    "#\n",
    "# Now plot the found separation line\n",
    "#\n",
    "\n",
    "plt.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY.ravel(), cmap=\"spring\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")\n",
    "pl_min, pl_max = plt.xlim()\n",
    "pl = np.linspace(pl_min, pl_max, 1000)\n",
    "plt.plot(pl, -(ThetaOpt[0] + ThetaOpt[1] * pl) / ThetaOpt[2])\n",
    "plt.xlim(pl_min, pl_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5tShF4KxNzS"
   },
   "source": [
    "## Task 4.2 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "executionInfo": {
     "elapsed": 2062,
     "status": "ok",
     "timestamp": 1603393880694,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "Qm3nPsVXxQMV",
    "outputId": "4ac84b5a-c2a1-4d3d-e453-746d1695ec91"
   },
   "outputs": [],
   "source": [
    "# Change logistic regression to include a weight penalty term as in ridge regression\n",
    "# Then plot the probabilities for different values of alpha\n",
    "# Hint: use the contour plot code from Assignent 1\n",
    "alphas = [0, 1e-2, 1.0]\n",
    "\n",
    "\n",
    "def logreg_loss_with_reg(Theta, X, Y, alpha=0.):\n",
    "    #\n",
    "    # Copy the logistic regression code and extend it with L2 regularization\n",
    "    #\n",
    "    TODO\n",
    "\n",
    "\n",
    "mesh_x, mesh_y = np.meshgrid(\n",
    "    np.linspace(IrisX[:, 1].min(), IrisX[:, 1].max(), 100),\n",
    "    np.linspace(IrisX[:, 2].min(), IrisX[:, 2].max(), 100),\n",
    ")\n",
    "mesh_data = np.hstack([np.ones(mesh_x.reshape(-1, 1).shape), mesh_x.reshape(-1, 1), mesh_y.reshape(-1, 1)])\n",
    "\n",
    "for alpha in alphas:\n",
    "  Theta0 = np.zeros((3, ))\n",
    "\n",
    "  #\n",
    "  # Call a solver\n",
    "  #\n",
    "  ThetaOpt, _, dic = sopt.fmin_l_bfgs_b(\n",
    "      lambda Theta: logreg_loss_with_reg(Theta, IrisX, IrisY, alpha=alpha), np.array(Theta0)\n",
    "  )\n",
    "  n_iter.append(dic['nit'])\n",
    "\n",
    "  #\n",
    "  # Now calculate probabilities for mesh_data\n",
    "  #\n",
    "  probs = TODO\n",
    "\n",
    "  #\n",
    "  # Plot the decision boundary\n",
    "  #\n",
    "  plt.contourf(mesh_x, mesh_y, probs.reshape(mesh_x.shape), cmap='spring')\n",
    "  plt.scatter(IrisX[:, 1], IrisX[:, 2], c=IrisY.ravel(), cmap=\"spring\", linewidths=1, edgecolors='black')\n",
    "  plt.colorbar()\n",
    "  plt.xlabel(\"petal_length\")\n",
    "  plt.ylabel(\"petal_width\")\n",
    "  pl_min, pl_max = plt.xlim()\n",
    "  plt.title(fr'$\\alpha$ = {alpha:.2e}')\n",
    "  plt.xlim(pl_min, pl_max)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rllhm5z2KXm"
   },
   "source": [
    "**TODO**: type here what has change with the change of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-U4Dz-NtdBj"
   },
   "source": [
    "# Problem 5 (Quantile Regression) [4p]\n",
    "\n",
    "  The least squares method results in estimates that approximate\n",
    "  the conditional mean of the response variable given certain values\n",
    "  of the predictor variables.  However, for many applications we are interested\n",
    "  in a median or other percentile rather than the mean of the response\n",
    "  variable. An approximation of a percentile can be achieved\n",
    "  indirectly by using the least squares method to fit a model,\n",
    "  assuming a gaussian distribution on residuals of this model and\n",
    "  calculating the required percentile of the gaussian distribution.\n",
    "  The problems with this commonly used technique appear when the\n",
    "  distribution of residuals does not follow a gaussian distribution.\n",
    "\n",
    "  The quantile regression\n",
    "  (https://en.wikipedia.org/wiki/Quantile_regression) aims at\n",
    "  directly estimating a value of the conditional percentile of the\n",
    "  response variable. It is often use in e.g. sales forecasting, where\n",
    "  we are interested in e.g. keeping enough items to have a 90% chance\n",
    "  to cover the demand. In quantile regression, instead of minimizing the mean squared error,\n",
    "  quantile regression minimizes a different cost function namely:\n",
    "  \\begin{equation}\n",
    "  f_{\\tau} = \\left\\{\n",
    "    \\begin{array}{rl}\n",
    "      \\tau\\cdot x & \\text{if } x \\geq 0,\\\\\n",
    "      -(1-\\tau)\\cdot x & \\text{if } x < 0,\n",
    "    \\end{array} \\right.\n",
    "  \\end{equation}\n",
    "  where $\\tau$ is the precentile of interest.\n",
    "\n",
    "\n",
    "Download the house pricing data set from\n",
    "    https://raw.githubusercontent.com/janchorowski/nn_assignments/nn18/assignment3/03-house-prices-outliers.csv. To\n",
    "    load it into Python, you can use the `pandas.read_csv`\n",
    "    function.\n",
    "\n",
    "    This data contains information about areas and prices of around\n",
    "    5000 houses offered for sale. Imagine you want to buy a 60 squared\n",
    "    meters flat. How much money do you need to have to be able to\n",
    "    choose between $75\\%$ of all flats offered for sale?\n",
    "\n",
    "1. **[1p]** Fit a linear regression model to the relation of a house price\n",
    "    on its area using the least squares method. Plot a historgram of the\n",
    "    residuals. Fit a Gaussian distribution to the residuals and overlay\n",
    "    it on the histogram. Calculate the 75 percentile of\n",
    "    this distribution (you can look it up in a percentile table, or\n",
    "    use the `scipy.stats.norm.ppf` function). Check the goodness of fit\n",
    "    of your model by calculating the ratio of prices below the\n",
    "    estimated 75 percentile. \n",
    "\n",
    "    How many samples are within the confidence interval? Is it too large or too small?\n",
    "\n",
    "2. **[1p]** Locate the outlying points and remove them from the dataset. Again check if the residuals look gaussian, and compute the 75th percentile.\n",
    "\n",
    "3. **[1.5p]** Fit a quantile regression model to fill data by minimizing the $f_{\\tau}$\n",
    "    function. To fit the model write a function returning the cost,\n",
    "    and its derivative (you can assume that the derivative is 0 at the\n",
    "    singular points) suitable for use with the L-BFGS solver. You\n",
    "    can start the solver from the least squares solution. Check the\n",
    "    goodness of fit as described above.\n",
    "\n",
    "    Now you can calculate the 75 percentile price value for a 60\n",
    "    squared meters flat using the more adequate model. \n",
    "\n",
    "4. **[0.5p]** Again, remove the outlying points and refit quantile regression. How much did the solution change? Why?\n",
    "    \n",
    "**Note**: Technically, we can not use a gradient-based\n",
    "    optimization method to minimize the $f_{\\tau}$ loss because it\n",
    "    doesn't have a derivative at $x=0$. One solution is to use a\n",
    "    subgradient method\n",
    "    https://en.wikipedia.org/wiki/Subderivative. It is also\n",
    "    possible to use a smooth loss function -- common variants are the\n",
    "    Huber loss (https://en.wikipedia.org/wiki/Huber_loss) or a\n",
    "    variant of the smooth approximation to the absolute value\n",
    "    $\\text{abs}(x) \\approx \\sqrt{x^2+\\epsilon}$ with a small\n",
    "    $\\epsilon$. However, just ignoring the singular points tends to\n",
    "    work well in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 170357,
     "status": "ok",
     "timestamp": 1603379460945,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "FTUb21Entja7",
    "outputId": "0a7796c4-494e-4618-b25e-9d302f06d1a4"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/janchorowski/nn_assignments/nn18/assignment3/03-house-prices-outliers.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 170347,
     "status": "ok",
     "timestamp": 1603379460946,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "Ty2H2H3_uqro",
    "outputId": "9ced5a44-203b-4fcb-d48c-9a26c65e5c62"
   },
   "outputs": [],
   "source": [
    "X = np.stack((np.ones_like(data.area), data.area)).T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 170339,
     "status": "ok",
     "timestamp": 1603379460948,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "5jEoxXb4vWmY",
    "outputId": "e15f8f78-d1e4-4f4e-c2a7-3161f2f286f6"
   },
   "outputs": [],
   "source": [
    "Y = np.asarray(data.price)[:, None]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 170329,
     "status": "ok",
     "timestamp": 1603379460950,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "XzVW0hnQoTSm",
    "outputId": "daf20fa3-9911-4771-f6c1-065ca73e432c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 170319,
     "status": "ok",
     "timestamp": 1603379460951,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "_CZcEHftoUmk",
    "outputId": "87e70561-5b75-4523-bbfd-51cc0bebe71d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 171258,
     "status": "ok",
     "timestamp": 1603379461900,
     "user": {
      "displayName": "Jan Chorowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqjVlbW0luD6bn2f_qryOD2Z2a5Wrer8PyDHk9RA=s64",
      "userId": "18100051538790809279"
     },
     "user_tz": -120
    },
    "id": "QE5v_9j9v4TW",
    "outputId": "aa8ea703-e457-4eb1-a7c5-cc0ed837bb50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AydiBTOZnw1V"
   },
   "source": [
    "# Problem 6 [1-4bp]\n",
    "\n",
    "Apply the [General Robust loss function](https://arxiv.org/abs/1701.03077) to the data from problem 5.\n",
    "\n",
    "Grading: you will get one bonus point for implementing the loss without the prior and trying a few values of $\\alpha$. Comparing and using  the provided implementation can give you another bonus point. Your own implementation of the adaptative variant gives 4 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1spEGdtSuQdj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
